---
title: "Transformer-based Bayesian Optimization"
collection: publications
index: 1
excerpt: '<b>Abstarct:</b> Bayesian optimization is widely used for black-box function optimization, excelling in global optimization and minimal sample requirements. It is valuable for complex, non-convex, and computationally expensive problems, prevalent in machine learning and optimization. Traditional Bayesian optimization relies on Gaussian processes, which face limitations in computational cost and scalability. We introduce a Transformer-based Bayesian optimization approach, <b>merging Gaussian process uncertainty estimation with neural network scalability</b>. This approach, incorporating self-attention mechanisms for better contextual integration, <b>outperforms traditional methods</b>, delivering similar optimization performance as Gaussian processes but with significantly <b>faster processing times (O(n1.41)</b> for PT-BO and O(n2.16) for GP-BO). Moreover, experiments show Transformer-based Bayesian optimization to be <b>more efficient for high-dimensional data or large datasets</b>, with higher optimization efficiency compared to GP-BO.'
date: 2023-06-24
# venue: 'IEEE Transactions on Image Processing vol.31'
# paperurl: 'https://ieeexplore.ieee.org/document/9776607'
citation: '<i>Advisor: <br>Yilin Mo, Associate Professor, Department of Automation, Tsinghua University <br>Yanan Sui, Associate Professor, Department of Aerospace Engineering, Tsinghua University</i>' 
img: "/images/TransformerBO.png"
---

